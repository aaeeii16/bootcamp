{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wektorowa reprezentacja tekstu\n",
    "\n",
    "W najprostszym przypadku budowanie reprezentacji wektorowej polega na zliczaniu ilości słów (ciągów). Każde słowo to atrybut a wartość w danym atrybucie to ilość wystąpniń tego słowa. \n",
    "\n",
    "\n",
    "## bag-of-words\n",
    "\n",
    "Najprostsza reprezentacja to reprezentacja bag-of-words: https://en.wikipedia.org/wiki/Bag-of-words_model. \n",
    "\n",
    "Kazdy wektor zapisany jest w rzadkiej reprezentacji jako że słownik może być duży a dokument mały."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    "\n",
    "* Weźmy tekst i podzielmy na tokeny. \n",
    "\n",
    "* Proszę stworzyć słownik słów oraz policzyć częstości każdego słowa. \n",
    "\n",
    "* Można użyć Counter - daje nam słownik: https://docs.python.org/2/library/collections.html#collections.Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to write a program in NLTK that breaks a corpus (a large collection of txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams. I need to write a program in NLTK that breaks a corpus\n"
     ]
    }
   ],
   "source": [
    "text = \"I need to write a program in NLTK that breaks a corpus (a large collection of txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams. I need to write a program in NLTK that breaks a corpus\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----słownik----\n",
      "{'need', 'large', 'into', 'a', 'in', 'fourgrams', 'collection', 'and', 'breaks', 'that', 'files', 'of', 'unigrams', ',', 'I', 'txt', 'NLTK', 'write', 'trigrams', 'fivegrams', 'bigrams', 'to', ')', 'program', 'corpus', '.', '('}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "unigrams = set(tokens)\n",
    "print(\"-----słownik----\")\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----licznik----\n",
      "Counter({'a': 5, ',': 3, 'I': 2, 'need': 2, 'to': 2, 'write': 2, 'program': 2, 'in': 2, 'NLTK': 2, 'that': 2, 'breaks': 2, 'corpus': 2, '(': 1, 'large': 1, 'collection': 1, 'of': 1, 'txt': 1, 'files': 1, ')': 1, 'into': 1, 'unigrams': 1, 'bigrams': 1, 'trigrams': 1, 'fourgrams': 1, 'and': 1, 'fivegrams': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"-----licznik----\")\n",
    "counts1 = Counter(tokens)\n",
    "print(counts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----klucze---\n",
      "dict_keys(['I', 'need', 'to', 'write', 'a', 'program', 'in', 'NLTK', 'that', 'breaks', 'corpus', '(', 'large', 'collection', 'of', 'txt', 'files', ')', 'into', 'unigrams', ',', 'bigrams', 'trigrams', 'fourgrams', 'and', 'fivegrams', '.'])\n",
      "----wartości--\n",
      "dict_values([2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"-----klucze---\")\n",
    "print(counts1.keys())\n",
    "print(\"----wartości--\")\n",
    "print(counts1.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--miejsce--\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(\"--miejsce--\")\n",
    "print(counts1['I'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Zostańmy przy unigramach. Proszę napisać reprezentację bag-of-words własnoręcznie. \n",
    "\n",
    "W tym celu:\n",
    "\n",
    " * tworzymy słownik słów na podstawie zdanego tekstu (metoda fit)\n",
    "\n",
    " * dla stokenizowanego dokumentu, tworzymy macierz częstości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'that', 'write', '(', 'NLTK', 'txt', 'unigrams', 'corpus', 'trigrams', 'program', ')', 'fourgrams', ',', 'bigrams', 'need', '.', 'I', 'large', 'collection', 'a', 'files', 'to', 'and', 'fivegrams', 'in', 'breaks', 'into', 'of'}\n",
      "--reprezentacja--\n",
      "[2. 2. 1. 2. 1. 1. 2. 1. 2. 1. 1. 3. 1. 2. 1. 2. 1. 1. 5. 1. 2. 1. 1. 2.\n",
      " 2. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(tokens):\n",
    "    words = set(tokens)\n",
    "    return words\n",
    "\n",
    "def transform(sentence_words, words):\n",
    "    # frequency word count\n",
    "    bag = np.zeros(len(words))\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag)\n",
    "\n",
    "words = fit(tokens)\n",
    "print(words)\n",
    "print(\"--reprezentacja--\")\n",
    "representation = transform(tokens, words)\n",
    "print(representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngrams\n",
    "\n",
    "Teksto można reprezentaować też jako ciągi czyli n-gramy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to write a program in NLTK that breaks a corpus (a large collection of txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams. I need to write a program in NLTK that breaks a corpus\n"
     ]
    }
   ],
   "source": [
    "text = \"I need to write a program in NLTK that breaks a corpus (a large collection of txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams. I need to write a program in NLTK that breaks a corpus\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nltk.word_tokenize(text)\n",
    "\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "fourgrams = ngrams(token,4)\n",
    "fivegrams = ngrams(token,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('I', 'need'): 2, ('need', 'to'): 2, ('to', 'write'): 2, ('write', 'a'): 2, ('a', 'program'): 2, ('program', 'in'): 2, ('in', 'NLTK'): 2, ('NLTK', 'that'): 2, ('that', 'breaks'): 2, ('breaks', 'a'): 2, ('a', 'corpus'): 2, ('corpus', '('): 1, ('(', 'a'): 1, ('a', 'large'): 1, ('large', 'collection'): 1, ('collection', 'of'): 1, ('of', 'txt'): 1, ('txt', 'files'): 1, ('files', ')'): 1, (')', 'into'): 1, ('into', 'unigrams'): 1, ('unigrams', ','): 1, (',', 'bigrams'): 1, ('bigrams', ','): 1, (',', 'trigrams'): 1, ('trigrams', ','): 1, (',', 'fourgrams'): 1, ('fourgrams', 'and'): 1, ('and', 'fivegrams'): 1, ('fivegrams', '.'): 1, ('.', 'I'): 1})\n"
     ]
    }
   ],
   "source": [
    "counts2 = Counter(bigrams)\n",
    "print(counts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('I', 'need', 'to'): 2, ('need', 'to', 'write'): 2, ('to', 'write', 'a'): 2, ('write', 'a', 'program'): 2, ('a', 'program', 'in'): 2, ('program', 'in', 'NLTK'): 2, ('in', 'NLTK', 'that'): 2, ('NLTK', 'that', 'breaks'): 2, ('that', 'breaks', 'a'): 2, ('breaks', 'a', 'corpus'): 2, ('a', 'corpus', '('): 1, ('corpus', '(', 'a'): 1, ('(', 'a', 'large'): 1, ('a', 'large', 'collection'): 1, ('large', 'collection', 'of'): 1, ('collection', 'of', 'txt'): 1, ('of', 'txt', 'files'): 1, ('txt', 'files', ')'): 1, ('files', ')', 'into'): 1, (')', 'into', 'unigrams'): 1, ('into', 'unigrams', ','): 1, ('unigrams', ',', 'bigrams'): 1, (',', 'bigrams', ','): 1, ('bigrams', ',', 'trigrams'): 1, (',', 'trigrams', ','): 1, ('trigrams', ',', 'fourgrams'): 1, (',', 'fourgrams', 'and'): 1, ('fourgrams', 'and', 'fivegrams'): 1, ('and', 'fivegrams', '.'): 1, ('fivegrams', '.', 'I'): 1, ('.', 'I', 'need'): 1})\n"
     ]
    }
   ],
   "source": [
    "counts3 = Counter(trigrams)\n",
    "print(counts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('I', 'need', 'to', 'write'): 2, ('need', 'to', 'write', 'a'): 2, ('to', 'write', 'a', 'program'): 2, ('write', 'a', 'program', 'in'): 2, ('a', 'program', 'in', 'NLTK'): 2, ('program', 'in', 'NLTK', 'that'): 2, ('in', 'NLTK', 'that', 'breaks'): 2, ('NLTK', 'that', 'breaks', 'a'): 2, ('that', 'breaks', 'a', 'corpus'): 2, ('breaks', 'a', 'corpus', '('): 1, ('a', 'corpus', '(', 'a'): 1, ('corpus', '(', 'a', 'large'): 1, ('(', 'a', 'large', 'collection'): 1, ('a', 'large', 'collection', 'of'): 1, ('large', 'collection', 'of', 'txt'): 1, ('collection', 'of', 'txt', 'files'): 1, ('of', 'txt', 'files', ')'): 1, ('txt', 'files', ')', 'into'): 1, ('files', ')', 'into', 'unigrams'): 1, (')', 'into', 'unigrams', ','): 1, ('into', 'unigrams', ',', 'bigrams'): 1, ('unigrams', ',', 'bigrams', ','): 1, (',', 'bigrams', ',', 'trigrams'): 1, ('bigrams', ',', 'trigrams', ','): 1, (',', 'trigrams', ',', 'fourgrams'): 1, ('trigrams', ',', 'fourgrams', 'and'): 1, (',', 'fourgrams', 'and', 'fivegrams'): 1, ('fourgrams', 'and', 'fivegrams', '.'): 1, ('and', 'fivegrams', '.', 'I'): 1, ('fivegrams', '.', 'I', 'need'): 1, ('.', 'I', 'need', 'to'): 1})\n"
     ]
    }
   ],
   "source": [
    "counts4 = Counter(fourgrams)\n",
    "print(counts4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('I', 'need', 'to', 'write', 'a'): 2, ('need', 'to', 'write', 'a', 'program'): 2, ('to', 'write', 'a', 'program', 'in'): 2, ('write', 'a', 'program', 'in', 'NLTK'): 2, ('a', 'program', 'in', 'NLTK', 'that'): 2, ('program', 'in', 'NLTK', 'that', 'breaks'): 2, ('in', 'NLTK', 'that', 'breaks', 'a'): 2, ('NLTK', 'that', 'breaks', 'a', 'corpus'): 2, ('that', 'breaks', 'a', 'corpus', '('): 1, ('breaks', 'a', 'corpus', '(', 'a'): 1, ('a', 'corpus', '(', 'a', 'large'): 1, ('corpus', '(', 'a', 'large', 'collection'): 1, ('(', 'a', 'large', 'collection', 'of'): 1, ('a', 'large', 'collection', 'of', 'txt'): 1, ('large', 'collection', 'of', 'txt', 'files'): 1, ('collection', 'of', 'txt', 'files', ')'): 1, ('of', 'txt', 'files', ')', 'into'): 1, ('txt', 'files', ')', 'into', 'unigrams'): 1, ('files', ')', 'into', 'unigrams', ','): 1, (')', 'into', 'unigrams', ',', 'bigrams'): 1, ('into', 'unigrams', ',', 'bigrams', ','): 1, ('unigrams', ',', 'bigrams', ',', 'trigrams'): 1, (',', 'bigrams', ',', 'trigrams', ','): 1, ('bigrams', ',', 'trigrams', ',', 'fourgrams'): 1, (',', 'trigrams', ',', 'fourgrams', 'and'): 1, ('trigrams', ',', 'fourgrams', 'and', 'fivegrams'): 1, (',', 'fourgrams', 'and', 'fivegrams', '.'): 1, ('fourgrams', 'and', 'fivegrams', '.', 'I'): 1, ('and', 'fivegrams', '.', 'I', 'need'): 1, ('fivegrams', '.', 'I', 'need', 'to'): 1, ('.', 'I', 'need', 'to', 'write'): 1})\n"
     ]
    }
   ],
   "source": [
    "counts5 = Counter(fivegrams)\n",
    "print(counts5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    "\n",
    "Do budowy reprezentacji dla wielu tekstów można użyć sklearn do tej samej czynności:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\", \"I'm 20 years old.\"]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard. I'm 20 years old.\"\n",
    "sentences = sent_tokenize(EXAMPLE_TEXT)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 9, 'mr': 12, 'smith': 18, 'how': 10, 'are': 2, 'you': 23, 'doing': 6, 'today': 20, 'the': 19, 'weather': 21, 'is': 11, 'great': 8, 'and': 1, 'python': 15, 'awesome': 3, 'sky': 17, 'pinkish': 14, 'blue': 4, 'shouldn': 16, 'eat': 7, 'cardboard': 5, '20': 0, 'years': 22, 'old': 13}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences)\n",
    "print( vectorizer.vocabulary_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 23)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 11)\t2\n",
      "  (1, 15)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 21)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 19)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 23)\t1\n",
      "  (4, 0)\t1\n",
      "  (4, 13)\t1\n",
      "  (4, 22)\t1\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vectorizer.transform(sentences)\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1]\n",
      " [0 1 0 1 0 0 0 0 1 0 0 2 0 0 0 1 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad:\n",
    "\n",
    "Możemy tutaj użyć preprocessing i tokenizacje domyślną, ale można też zdefiniować własną.\n",
    "\n",
    "Proszę użyć wcześniej zdefiniowane funkcje do przetwarzania tekstu w naszym transfromerze - zobacz:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "a następnie stworzyć reprezentację **bag-of-words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "with\n",
      "before\n",
      "now\n",
      "~\n",
      "once\n",
      "himself\n",
      "of\n",
      "don't\n",
      "=\n",
      "your\n",
      "re\n",
      "those\n",
      "m\n",
      "own\n",
      "my\n",
      "\\\n",
      "both\n",
      "mightn't\n",
      "as\n",
      "these\n",
      "needn\n",
      "such\n",
      "than\n",
      "very\n",
      "ours\n",
      "herself\n",
      "*\n",
      "`\n",
      "am\n",
      "off\n",
      "or\n",
      "haven\n",
      "mustn\n",
      ")\n",
      "but\n",
      "having\n",
      "be\n",
      "it\n",
      "below\n",
      "same\n",
      "needn't\n",
      "}\n",
      "about\n",
      "were\n",
      "more\n",
      "hadn't\n",
      "hasn't\n",
      "won't\n",
      "]\n",
      "ain\n",
      "{\n",
      "haven't\n",
      "!\n",
      "hasn\n",
      "over\n",
      "only\n",
      "you're\n",
      "we\n",
      "their\n",
      "just\n",
      "her\n",
      "so\n",
      "from\n",
      "s\n",
      "that'll\n",
      "down\n",
      "should\n",
      "shouldn't\n",
      "\"\n",
      "other\n",
      ";\n",
      "nor\n",
      "out\n",
      "have\n",
      "and\n",
      "while\n",
      "there\n",
      "has\n",
      "not\n",
      "mustn't\n",
      ".\n",
      "into\n",
      "couldn\n",
      "where\n",
      "why\n",
      "them\n",
      "itself\n",
      "isn\n",
      "yours\n",
      "@\n",
      "here\n",
      "this\n",
      "do\n",
      "doesn't\n",
      "hadn\n",
      "$\n",
      ":\n",
      "wasn\n",
      "what\n",
      "|\n",
      "doing\n",
      "no\n",
      "yourselves\n",
      "whom\n",
      "it's\n",
      "further\n",
      "o\n",
      "he\n",
      "you'd\n",
      "between\n",
      "after\n",
      "when\n",
      "'\n",
      "she\n",
      "ll\n",
      "him\n",
      "didn\n",
      "d\n",
      "she's\n",
      "theirs\n",
      "myself\n",
      "against\n",
      "most\n",
      "yourself\n",
      "during\n",
      "wouldn\n",
      "aren\n",
      "%\n",
      "didn't\n",
      "ma\n",
      "-\n",
      "at\n",
      "a\n",
      "had\n",
      "?\n",
      "they\n",
      "who\n",
      "you'll\n",
      "through\n",
      "me\n",
      "t\n",
      "doesn\n",
      "how\n",
      "<\n",
      "[\n",
      "up\n",
      "mightn\n",
      "the\n",
      "you've\n",
      "some\n",
      "couldn't\n",
      "by\n",
      "is\n",
      "to\n",
      "all\n",
      "_\n",
      "&\n",
      ">\n",
      "aren't\n",
      "/\n",
      "weren\n",
      "ourselves\n",
      "can\n",
      "you\n",
      "its\n",
      "above\n",
      "isn't\n",
      "wouldn't\n",
      "under\n",
      "themselves\n",
      "any\n",
      "will\n",
      "if\n",
      "in\n",
      "don\n",
      "should've\n",
      "that\n",
      "few\n",
      "wasn't\n",
      "then\n",
      "again\n",
      "an\n",
      "been\n",
      "because\n",
      "did\n",
      "i\n",
      ",\n",
      "until\n",
      "on\n",
      "hers\n",
      "shouldn\n",
      "weren't\n",
      "shan\n",
      "#\n",
      "+\n",
      "won\n",
      "shan't\n",
      "^\n",
      "y\n",
      "ve\n",
      "his\n",
      "which\n",
      "our\n",
      "each\n",
      "was\n",
      "are\n",
      "does\n",
      "for\n",
      "too\n",
      "being\n",
      "Hello Mr. Smith, how are you doing today?\n",
      "The weather is great, and Python is awesome.\n",
      "The sky is pinkish-blue.\n",
      "You shouldn't eat cardboard.\n",
      "I'm 20 years old.\n",
      "{'hello': 7, 'mr.': 8, 'smith': 14, 'today': 15, 'weather': 16, 'great': 6, 'python': 12, 'awesom': 3, 'sky': 13, 'pinkish-blu': 11, \"n't\": 9, 'eat': 5, 'cardboard': 4, 'I': 2, \"'m\": 0, '20': 1, 'year': 17, 'old': 10}\n",
      "Hello Mr. Smith, how are you doing today?\n",
      "The weather is great, and Python is awesome.\n",
      "The sky is pinkish-blue.\n",
      "You shouldn't eat cardboard.\n",
      "I'm 20 years old.\n",
      "[[0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#tokenizer działa na tokenach już\n",
    "def stemming_tokenizer(text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in tokenized]\n",
    "\n",
    "#preprocessor działa na całym dokumencie\n",
    "def my_preprocessing(word):\n",
    "    print(word)\n",
    "    return word\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = my_preprocessing, tokenizer=stemming_tokenizer, stop_words=stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "vectorizer.fit(sentences)\n",
    "print( vectorizer.vocabulary_ )\n",
    "print(vectorizer.transform(sentences).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    " \n",
    "Proszę wykonać podstawową tekenizację biorąc pod uwagę:\n",
    "\n",
    " * bierzemy kolejne artykuły i dzieli go na tokeny\n",
    " * bierzemy listę tokenów i usuwamy punktory\n",
    " * bierzemy listę tokenów i usuwa liczby\n",
    " * bierzemy listę tokenów i zamieniamy na małe litery\n",
    " \n",
    "a następnie stworzyć reprezentację **bag-of-words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 5, 'mr.': 6, 'smith': 12, 'today': 13, 'weather': 14, 'great': 4, 'python': 10, 'awesom': 1, 'sky': 11, 'pinkish-blu': 9, \"n't\": 7, 'eat': 3, 'cardboard': 2, \"'m\": 0, 'year': 15, 'old': 8}\n",
      "[[0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "def sen2token(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    remove_list = string.punctuation\n",
    "    return [w for w in words if not w in set(remove_list)]\n",
    "\n",
    "def remove_nummbers(words):\n",
    "    return [w for w in words if not w.isdigit()]\n",
    "\n",
    "def to_lower(words):\n",
    "    return [w.lower() for w in words]\n",
    "\n",
    "def stemming_tokenizer(words):\n",
    "    words = sen2token(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_nummbers(words)\n",
    "    words = to_lower(words)\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "#preprocessor działa na całym dokumencie\n",
    "def my_preprocessing(word):\n",
    "    print(word)\n",
    "    return word\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=stemming_tokenizer, stop_words=stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "vectorizer.fit(sentences)\n",
    "print( vectorizer.vocabulary_ )\n",
    "print(vectorizer.transform(sentences).todense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Vectorizer buduje nam słownik, czyli definiuje poszczególne wymiary. Mając taki słownik możemy transformować nowe zdania.\n",
    "\n",
    "Jaką reprezentację będą miały zdania:\n",
    "\n",
    "\n",
    " * \"The weather is awesome\"\n",
    " \n",
    " * \"You shouldn't eat 20 20 30 cardboards\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather is awesome\n",
      "You shouldn't eat 20 20 30 cardboards\n",
      "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_sentences_1 = [\"The weather is awesome\", \"You shouldn't eat 20 20 30 cardboards\"]\n",
    "\n",
    "print(vectorizer.transform(new_sentences_1).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    "\n",
    "Jaką reprezentację będą miały zdania zawierające słowo nie występujące wcześniej (\"apples\"):\n",
    "\n",
    "\n",
    " * \"You shouldn't eat apples\"\n",
    " * \"Apples apples apples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_sentences_2 = [\"You shouldn't eat eat apples\", \"Apples apples apples\"]\n",
    "\n",
    "print(vectorizer.transform(new_sentences_2).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
