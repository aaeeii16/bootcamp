{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities, matutils\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "LSI pozwala w pewnym sensie znajdowac kluczowe tematy w tekście i dla danego dokumentu określać najbliższy mu temat. Jest to podejście geometryczne. \n",
    "\n",
    "LDA jest podejściem probabilistycznym do modelowania tematów. Jest bardziej dokładny choć wolniejszy.\n",
    "\n",
    "# Model LDA - Latent Dirichlet Allocation (ukryta alokacja Dirichleta)\n",
    "\n",
    "\n",
    "Motywacja: przedstawienie tekstu jako mieszanki tematów.\n",
    "\n",
    "\n",
    "Temat - rozkład prawdopodobieństwa na zbiorze słów.\n",
    "\n",
    "\n",
    "Przykład:\n",
    "*  <s>Mam</s> gorączkę <s>i</s> katar.\n",
    "* Graliśmy <s>w</s> siatkówkę.\n",
    "* Grając <s>w</s> piłkę, wzmacniamy organizm.\n",
    "\n",
    "\n",
    "Ile \"tematów\" widzimy?\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Intuicyjnie: dwa tematy: \"sport\" oraz \"zdrowie\".\n",
    "* Pierwsze zdanie = 100% zdrowie\n",
    "* Drugie zdanie = 100% sport\n",
    "* Trzecie zdanie = 50% sport + 50% zdrowie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA to model probabilistyczny, który wykorzystuje dwie wartości prawdopodobieństwa: \n",
    "\n",
    "* P (słowo | tematy) \n",
    "* P (tematy | dokumenty) \n",
    "\n",
    "do stworzenia klastrów.\n",
    "\n",
    "\n",
    "Proces budowy modelu LDA zakłada więc:\n",
    "* identyfikację tematów reprezentowanych przez dokumenty korpusu,\n",
    "* oszacowanie rozkładu prawdopodobieństw wystąpienia wyrazów dla każdego z tematów (stosowany jest rozkład Dirichleta),\n",
    "* oszacowanie rozkładu prawdopodobieństw wystąpienia tematów w rozpatrywanych dokumentach (stosowany jest również rozkład Dirichleta).\n",
    "\n",
    "Wykorzystując metodę identyfikacji słów kluczowych opartą na ukrytej alokacji Dirichleta można:\n",
    "* dla każdego dokumentu obliczyć prawdopodobieństwo wystąpienia w nim każdego z tematów;\n",
    "* dla każdego tematu obliczyć prawdopodobieństwo wystąpienia słów;\n",
    "* obliczyć prawdopodobieństwa wystąpienia poszczególnych słów w dokumencie (jako iloczyny dwóch wyżej wymienionych prawdopodobieństw) \n",
    "\n",
    "# Generatywność\n",
    "\n",
    "Mając prawdopodobieństwa $p_1 = P (słowo | tematy) $, $p_2 = P (tematy | dokumenty)$ możemy wygeneraować dokument:\n",
    "\n",
    "- wybieramy (losujemy) prawdopodobieństwo przynależności dokumentu do tematu (z $p_1$) - dokuemnt zawsze należy do wielu tematów\n",
    "\n",
    "- generujemy słowa w dokumencie - najpierw losujemy do jakiego tematu należy słowo, a potem generujemy słowo z tego tematu (z $p_2$)\n",
    "\n",
    "# Uczenie\n",
    "\n",
    "1. Idziemy przez wszystkie słowa i wszystkie dokumentu i losowo przyporządkowujemy je do tematów.\n",
    "2. Iteracyjnie idzemy: bierzemy dokuemnt $d$  i słowo $w$ i przyporządkowujemy je do najlepiej pasującego tematu - maksymalizującego $P (słowo | tematy) \n",
    "* P (tematy | dokumenty) $\n",
    "3. Po przejsciu przez wszystkie dokumenty aktualizujemy przydział słów do tematów i tematów do dokumentów i wracamy do punktu 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Na początek wczytujemy korpus z dysku. Użyjemy przykład stworzonego w poprzednim notebook-u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = [\"Romeo and Juliet\",\n",
    "#          \"Juliet: O happy dagger\",\n",
    "#          \"Romeo died by dagger\",\n",
    "#          \"'Live free or die', that’s the New-Hampshire’s motto\",\n",
    "#          \"Did you know, New-Hampshire is in New-England\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used files generated from first tutorial\n"
     ]
    }
   ],
   "source": [
    "if (os.path.exists(\"tmp/deerwester.dict\")):\n",
    "    dictionary = corpora.Dictionary.load('tmp/deerwester.dict')\n",
    "    corpus = corpora.MmCorpus('tmp/deerwester.mm')\n",
    "    print(\"Used files generated from first tutorial\")\n",
    "else:\n",
    "    print(\"Please run first tutorial to generate data set\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 1.0)]\n",
      "[(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0)]\n",
      "[(1, 1.0), (2, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0)]\n",
      "[(15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "for d in corpus:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "scipy_csc_matrix = matutils.corpus2csc(corpus)\n",
    "print(scipy_csc_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'juliet': 0, 'romeo': 1, 'dagger': 2, 'happy': 3, 'juliet:': 4, 'o': 5, 'by': 6, 'died': 7, \"'live\": 8, \"die',\": 9, 'free': 10, 'motto': 11, 'new-hampshire’s': 12, 'or': 13, 'that’s': 14, 'did': 15, 'is': 16, 'know,': 17, 'new-england': 18, 'new-hampshire': 19, 'you': 20}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Budujemy model LDA\n",
    "Budujemy model LDA i transformujemy dane\n",
    "\n",
    "* **num_topics=2** oznacza ilość modelowanych tematów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.22476293), (1, 0.7752371)]\n",
      "[(0, 0.14855596), (1, 0.85144407)]\n",
      "[(0, 0.1534796), (1, 0.84652036)]\n",
      "[(0, 0.06724484), (1, 0.9327552)]\n",
      "[(0, 0.9248575), (1, 0.075142525)]\n"
     ]
    }
   ],
   "source": [
    "model = models.LdaModel(corpus, id2word=dictionary, num_topics=2)\n",
    "corpus_lda = model[corpus] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "for d in corpus_lda:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dla każdego dokumentu dostajemy prawdopodobieństwo przynależności dokumentu do danego tematu.\n",
    "\n",
    "Możemy też zobaczyć z czego składają się tematy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.072*\"you\" + 0.072*\"is\" + 0.071*\"know,\" + 0.071*\"new-hampshire\" + 0.071*\"did\" + 0.071*\"new-england\" + 0.058*\"dagger\" + 0.054*\"romeo\" + 0.050*\"by\" + 0.045*\"juliet:\"'),\n",
       " (1,\n",
       "  '0.080*\"romeo\" + 0.077*\"dagger\" + 0.061*\"motto\" + 0.061*\"or\" + 0.060*\"\\'live\" + 0.060*\"new-hampshire’s\" + 0.060*\"that’s\" + 0.060*\"die\\',\" + 0.059*\"free\" + 0.050*\"o\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.072*\"you\" + 0.072*\"is\" + 0.071*\"know,\" + 0.071*\"new-hampshire\" + 0.071*\"did\" + 0.071*\"new-england\" + 0.058*\"dagger\" + 0.054*\"romeo\" + 0.050*\"by\" + 0.045*\"juliet:\"'),\n",
       " (1,\n",
       "  '0.080*\"romeo\" + 0.077*\"dagger\" + 0.061*\"motto\" + 0.061*\"or\" + 0.060*\"\\'live\" + 0.060*\"new-hampshire’s\" + 0.060*\"that’s\" + 0.060*\"die\\',\" + 0.059*\"free\" + 0.050*\"o\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.072*\"you\" + 0.072*\"is\" + 0.071*\"know,\" + 0.071*\"new-hampshire\"'),\n",
       " (1, '0.080*\"romeo\" + 0.077*\"dagger\" + 0.061*\"motto\" + 0.061*\"or\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics(num_topics=2, num_words=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Chcemy posortować słowa każdego tematu i wybrać 5 najważniejszych - co można powiedzieć o tematach?\n",
    "\n",
    "Proszę zobaczyć na funkcje typu get_topics(), get_term_topics(...): https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you\n",
      "is\n",
      "know,\n",
      "new-hampshire\n",
      "did\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "topics = np.argsort(model.get_topics()[0,:])[::-1] #::-1 sortowanie w odwrotnej kolejności\n",
    "for x in topics[:5]:\n",
    "    print(dictionary[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 0.071840286),\n",
       " (16, 0.07153516),\n",
       " (17, 0.071404055),\n",
       " (19, 0.07138407),\n",
       " (15, 0.071238615),\n",
       " (18, 0.070652395),\n",
       " (2, 0.05751301),\n",
       " (1, 0.054392435),\n",
       " (6, 0.05003244),\n",
       " (4, 0.04501523)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_topic_terms(topicid=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 0.071840286),\n",
       " ('is', 0.07153516),\n",
       " ('know,', 0.071404055),\n",
       " ('new-hampshire', 0.07138407),\n",
       " ('did', 0.071238615),\n",
       " ('new-england', 0.070652395),\n",
       " ('dagger', 0.05751301),\n",
       " ('romeo', 0.054392435),\n",
       " ('by', 0.05003244),\n",
       " ('juliet:', 0.04501523)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic2_terms = model.get_topic_terms(topicid=0)\n",
    "topic2_words = [\n",
    "    (dictionary.get(i), j)\n",
    "    for i,j in topic2_terms\n",
    "]\n",
    "topic2_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 0.071840286),\n",
       " ('is', 0.07153516),\n",
       " ('know,', 0.071404055),\n",
       " ('new-hampshire', 0.07138407),\n",
       " ('did', 0.071238615),\n",
       " ('new-england', 0.070652395),\n",
       " ('dagger', 0.05751301),\n",
       " ('romeo', 0.054392435),\n",
       " ('by', 0.05003244),\n",
       " ('juliet:', 0.04501523)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    (dictionary.get(i), j)\n",
    "    for i,j in model.get_topic_terms(topicid=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    "\n",
    "Proszę posortować zdania najbardziej pasujące do danego tematu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 1.0)]\n",
      "[(0, 1.0), (1, 1.0)]\n",
      "[(1, 1.0), (2, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0)]\n",
      "[(8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "top_inex = 0\n",
    "\n",
    "numpy_corpus = gensim.matutils.corpus2dense(corpus_lda, num_terms=2)\n",
    "docs = np.argsort(numpy_corpus[top_inex,:])[::-1]\n",
    "for x in docs[:5]:\n",
    "    print(corpus[x])\n",
    "    \n",
    "#trzeba by wypisać raczej zdania niż ich reprezentacje bag-of-words, ale tu nie mam dostepu do tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0)]\n",
      "[(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0)]\n",
      "[(1, 1.0), (2, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(0, 1.0), (1, 1.0)]\n",
      "[(15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "top_inex = 1\n",
    "\n",
    "numpy_corpus = gensim.matutils.corpus2dense(corpus_lda, num_terms=2)\n",
    "docs = np.argsort(numpy_corpus[top_inex,:])[::-1]\n",
    "for x in docs[:5]:\n",
    "    print(corpus[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you know, New-Hampshire is in New-England\n",
      "Romeo and Juliet\n",
      "Romeo died by dagger\n",
      "Juliet: O happy dagger\n",
      "'Live free or die', that’s the New-Hampshire’s motto\n"
     ]
    }
   ],
   "source": [
    "documents = [\"Romeo and Juliet\",\n",
    "         \"Juliet: O happy dagger\",\n",
    "         \"Romeo died by dagger\",\n",
    "         \"'Live free or die', that’s the New-Hampshire’s motto\",\n",
    "         \"Did you know, New-Hampshire is in New-England\"]\n",
    "\n",
    "top_inex = 0\n",
    "\n",
    "numpy_corpus = gensim.matutils.corpus2dense(corpus_lda, num_terms=2)\n",
    "docs = np.argsort( numpy_corpus[top_inex,:] )[::-1]\n",
    "for x in docs[:5]:\n",
    "    print(documents[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Live free or die', that’s the New-Hampshire’s motto\n",
      "Juliet: O happy dagger\n",
      "Romeo died by dagger\n",
      "Romeo and Juliet\n",
      "Did you know, New-Hampshire is in New-England\n"
     ]
    }
   ],
   "source": [
    "top_inex = 1\n",
    "\n",
    "numpy_corpus = gensim.matutils.corpus2dense(corpus_lda, num_terms=2)\n",
    "docs = np.argsort(numpy_corpus[top_inex,:])[::-1]\n",
    "for x in docs[:5]:\n",
    "    print(documents[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad. \n",
    "Sprawdzić do jakiego tematu pasuje nowy dokument i jakie są mu najbliższe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"died dagger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.22685523), (1, 0.7731448)]\n"
     ]
    }
   ],
   "source": [
    "doc_rep = dictionary.doc2bow(doc.split(' '))\n",
    "# print(doc_rep)\n",
    "doc_assignments = model[doc_rep]\n",
    "print(doc_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.9999944), (1, 0.99366546), (2, 0.9943736), (3, 0.9773077), (4, 0.35832596)]\n",
      "[(0, 0.9999944), (2, 0.9943736), (1, 0.99366546), (3, 0.9773077), (4, 0.35832596)]\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(corpus_lda)\n",
    "\n",
    "sims = index[doc_assignments]\n",
    "print(list(enumerate(sims)))\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Wizualizacja modelu LDA:\n",
    "\n",
    "pyLDAvis\n",
    "\n",
    "http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb\n",
    "\n",
    "\n",
    "* Czerwone słupki reprezentują częstotliwość słów w danym temacie (proporcjonalnie do $P (słowo \\ | \\ tematy) $), \n",
    "* Niebieskie słupki reprezentują częstotliwość tematów w całym korpusie (proporcjonalnie do $P(tematy \\ | \\ dokumenty)$. \n",
    "\n",
    "Zmień wartość $\\lambda$, aby dostosować rankingi słów: \n",
    " * małe wartości $\\lambda$ (blisko 0) podkreślają potencjalnie rzadkie, ale ekskluzywne słowa dla wybranego tematu\n",
    " * duże wartości $\\lambda$ (blisko 1) podkreślają częste, ale niekoniecznie ekskluzywne słowa dla wybranego tematu. \n",
    " \n",
    "W http://www.kennyshirley.com/LDAvis/ sugeruje się żeby ustawiać $\\lambda$ w pobliżu 0.6. Podobno pomaga to użytkownikom w interpretacji tematu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis.gensim.prepare??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el13596555095733288447750817\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el13596555095733288447750817_data = {\"mdsDat\": {\"x\": [0.032103281468153, -0.032103281468153], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [55.20766830444336, 44.792327880859375]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"Freq\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.1975171566009521, 0.7442928552627563, 0.740874171257019, 0.7371100187301636, 0.7355400323867798, 0.7342511415481567, 0.7307618856430054, 0.729809582233429, 0.7273661494255066, 0.7157350182533264, 0.7141525745391846, 0.9465117454528809, 0.4480045735836029, 0.3886272609233856, 0.3883146345615387, 0.353452205657959, 0.34526610374450684, 0.33720025420188904, 0.33330270648002625, 0.3324098289012909, 0.31726545095443726, 0.7254616618156433, 0.710476279258728, 0.7095928192138672, 0.7057357430458069, 0.6977542638778687, 0.689654529094696, 0.6551579236984253, 0.6548486948013306, 0.5960941314697266, 0.6225165724754333, 0.3327397108078003, 0.33117374777793884, 0.3196646273136139, 0.31724727153778076, 0.3163048326969147, 0.3128516972064972, 0.31157681345939636, 0.3100232779979706, 0.3062986731529236, 0.302916020154953, 0.3741452395915985], \"Term\": [\"you\", \"new-hampshire\", \"new-england\", \"did\", \"know,\", \"is\", \"happy\", \"juliet:\", \"romeo\", \"o\", \"that\\u2019s\", \"or\", \"'live\", \"new-hampshire\\u2019s\", \"free\", \"motto\", \"die',\", \"by\", \"died\", \"juliet\", \"dagger\", \"romeo\", \"that\\u2019s\", \"or\", \"'live\", \"new-hampshire\\u2019s\", \"free\", \"motto\", \"die',\", \"by\", \"died\", \"juliet\", \"dagger\", \"o\", \"juliet:\", \"happy\", \"is\", \"know,\", \"did\", \"new-england\", \"new-hampshire\", \"you\", \"you\", \"new-hampshire\", \"new-england\", \"did\", \"know,\", \"is\", \"happy\", \"juliet:\", \"o\", \"dagger\", \"juliet\", \"died\", \"by\", \"die',\", \"motto\", \"free\", \"new-hampshire\\u2019s\", \"'live\", \"or\", \"that\\u2019s\", \"romeo\"], \"Total\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.571662425994873, 1.0472089052200317, 1.0471727848052979, 1.0471333265304565, 1.0471168756484985, 1.0471028089523315, 1.0470666885375977, 1.0470569133758545, 1.0470308065414429, 1.0469087362289429, 1.0468922853469849, 1.569028377532959, 1.0440987348556519, 1.0434759855270386, 1.0434725284576416, 1.0431067943572998, 1.0430203676223755, 1.0429359674453735, 1.0428955554962158, 1.0428861379623413, 1.0427271127700806, 1.0427271127700806, 1.0428861379623413, 1.0428955554962158, 1.0429359674453735, 1.0430203676223755, 1.0431067943572998, 1.0434725284576416, 1.0434759855270386, 1.0440987348556519, 1.569028377532959, 1.0468922853469849, 1.0469087362289429, 1.0470308065414429, 1.0470569133758545, 1.0470666885375977, 1.0471028089523315, 1.0471168756484985, 1.0471333265304565, 1.0471727848052979, 1.0472089052200317, 1.571662425994873], \"loglift\": [21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.3222000002861023, 0.2526000142097473, 0.24799999594688416, 0.24300000071525574, 0.24089999496936798, 0.23909999430179596, 0.23440000414848328, 0.23309999704360962, 0.2298000007867813, 0.21379999816417694, 0.21160000562667847, 0.08860000222921371, -0.25200000405311584, -0.3935999870300293, -0.3944000005722046, -0.48809999227523804, -0.5115000009536743, -0.5350000262260437, -0.5465999841690063, -0.5493000149726868, -0.59579998254776, 0.44029998779296875, 0.41929998993873596, 0.4180999994277954, 0.41260001063346863, 0.4011000096797943, 0.38940000534057617, 0.3377000093460083, 0.33719998598098755, 0.2425999939441681, -0.12129999697208405, -0.34310001134872437, -0.34779998660087585, -0.3833000063896179, -0.39089998602867126, -0.3939000070095062, -0.4049000144004822, -0.4090000092983246, -0.414000004529953, -0.4262000024318695, -0.4372999966144562, -0.632099986076355], \"logprob\": [21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.3612000942230225, -2.836699962615967, -2.841399908065796, -2.846400022506714, -2.848599910736084, -2.8503000736236572, -2.85509991645813, -2.8564000129699707, -2.859800100326538, -2.8759000301361084, -2.8780999183654785, -2.596400022506714, -3.344399929046631, -3.486599922180176, -3.4874000549316406, -3.581399917602539, -3.6048998832702637, -3.628499984741211, -3.6401000022888184, -3.6428000926971436, -3.6893999576568604, -2.6533000469207764, -2.6742000579833984, -2.6754000186920166, -2.6809000968933105, -2.692199945449829, -2.703900098800659, -2.755199909210205, -2.75570011138916, -2.8496999740600586, -2.806299924850464, -3.432800054550171, -3.4375, -3.4728000164031982, -3.4804000854492188, -3.4834001064300537, -3.4944000244140625, -3.498500108718872, -3.503499984741211, -3.5155999660491943, -3.526700019836426, -3.315500020980835]}, \"token.table\": {\"Topic\": [1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2], \"Freq\": [0.9549882411956787, 0.9550817608833313, 0.6373370885848999, 0.6373370885848999, 0.9588316082954407, 0.9550579190254211, 0.9551931023597717, 0.9550160765647888, 0.9583386182785034, 0.9586746096611023, 0.9552081227302551, 0.9583353996276855, 0.9587540626525879, 0.9550489783287048, 0.9588688015937805, 0.958877444267273, 0.9550032019615173, 0.9577638506889343, 0.9549522399902344, 0.6362689733505249, 0.9549192786216736, 0.9590237140655518], \"Term\": [\"'live\", \"by\", \"dagger\", \"dagger\", \"did\", \"die',\", \"died\", \"free\", \"happy\", \"is\", \"juliet\", \"juliet:\", \"know,\", \"motto\", \"new-england\", \"new-hampshire\", \"new-hampshire\\u2019s\", \"o\", \"or\", \"romeo\", \"that\\u2019s\", \"you\"]}, \"R\": 21, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el13596555095733288447750817\", ldavis_el13596555095733288447750817_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el13596555095733288447750817\", ldavis_el13596555095733288447750817_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el13596555095733288447750817\", ldavis_el13596555095733288447750817_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x    y  topics  cluster       Freq\n",
       "topic                                           \n",
       "0      0.032103  0.0       1        1  55.207668\n",
       "1     -0.032103  0.0       2        1  44.792328, topic_info=     Category      Freq             Term     Total  loglift  logprob\n",
       "term                                                                \n",
       "20    Default  1.000000              you  1.000000  21.0000  21.0000\n",
       "19    Default  1.000000    new-hampshire  1.000000  20.0000  20.0000\n",
       "18    Default  1.000000      new-england  1.000000  19.0000  19.0000\n",
       "15    Default  1.000000              did  1.000000  18.0000  18.0000\n",
       "17    Default  1.000000            know,  1.000000  17.0000  17.0000\n",
       "16    Default  1.000000               is  1.000000  16.0000  16.0000\n",
       "3     Default  1.000000            happy  1.000000  15.0000  15.0000\n",
       "4     Default  1.000000          juliet:  1.000000  14.0000  14.0000\n",
       "1     Default  1.000000            romeo  1.000000  13.0000  13.0000\n",
       "5     Default  1.000000                o  1.000000  12.0000  12.0000\n",
       "14    Default  1.000000           that’s  1.000000  11.0000  11.0000\n",
       "13    Default  1.000000               or  1.000000  10.0000  10.0000\n",
       "8     Default  1.000000            'live  1.000000   9.0000   9.0000\n",
       "12    Default  1.000000  new-hampshire’s  1.000000   8.0000   8.0000\n",
       "10    Default  1.000000             free  1.000000   7.0000   7.0000\n",
       "11    Default  1.000000            motto  1.000000   6.0000   6.0000\n",
       "9     Default  1.000000            die',  1.000000   5.0000   5.0000\n",
       "6     Default  1.000000               by  1.000000   4.0000   4.0000\n",
       "7     Default  1.000000             died  1.000000   3.0000   3.0000\n",
       "0     Default  1.000000           juliet  1.000000   2.0000   2.0000\n",
       "2     Default  1.000000           dagger  1.000000   1.0000   1.0000\n",
       "1      Topic1  1.197517            romeo  1.571662   0.3222  -2.3612\n",
       "14     Topic1  0.744293           that’s  1.047209   0.2526  -2.8367\n",
       "13     Topic1  0.740874               or  1.047173   0.2480  -2.8414\n",
       "8      Topic1  0.737110            'live  1.047133   0.2430  -2.8464\n",
       "12     Topic1  0.735540  new-hampshire’s  1.047117   0.2409  -2.8486\n",
       "10     Topic1  0.734251             free  1.047103   0.2391  -2.8503\n",
       "11     Topic1  0.730762            motto  1.047067   0.2344  -2.8551\n",
       "9      Topic1  0.729810            die',  1.047057   0.2331  -2.8564\n",
       "6      Topic1  0.727366               by  1.047031   0.2298  -2.8598\n",
       "...       ...       ...              ...       ...      ...      ...\n",
       "5      Topic1  0.448005                o  1.044099  -0.2520  -3.3444\n",
       "4      Topic1  0.388627          juliet:  1.043476  -0.3936  -3.4866\n",
       "3      Topic1  0.388315            happy  1.043473  -0.3944  -3.4874\n",
       "16     Topic1  0.353452               is  1.043107  -0.4881  -3.5814\n",
       "17     Topic1  0.345266            know,  1.043020  -0.5115  -3.6049\n",
       "15     Topic1  0.337200              did  1.042936  -0.5350  -3.6285\n",
       "18     Topic1  0.333303      new-england  1.042896  -0.5466  -3.6401\n",
       "19     Topic1  0.332410    new-hampshire  1.042886  -0.5493  -3.6428\n",
       "20     Topic1  0.317265              you  1.042727  -0.5958  -3.6894\n",
       "20     Topic2  0.725462              you  1.042727   0.4403  -2.6533\n",
       "19     Topic2  0.710476    new-hampshire  1.042886   0.4193  -2.6742\n",
       "18     Topic2  0.709593      new-england  1.042896   0.4181  -2.6754\n",
       "15     Topic2  0.705736              did  1.042936   0.4126  -2.6809\n",
       "17     Topic2  0.697754            know,  1.043020   0.4011  -2.6922\n",
       "16     Topic2  0.689655               is  1.043107   0.3894  -2.7039\n",
       "3      Topic2  0.655158            happy  1.043473   0.3377  -2.7552\n",
       "4      Topic2  0.654849          juliet:  1.043476   0.3372  -2.7557\n",
       "5      Topic2  0.596094                o  1.044099   0.2426  -2.8497\n",
       "2      Topic2  0.622517           dagger  1.569028  -0.1213  -2.8063\n",
       "0      Topic2  0.332740           juliet  1.046892  -0.3431  -3.4328\n",
       "7      Topic2  0.331174             died  1.046909  -0.3478  -3.4375\n",
       "6      Topic2  0.319665               by  1.047031  -0.3833  -3.4728\n",
       "9      Topic2  0.317247            die',  1.047057  -0.3909  -3.4804\n",
       "11     Topic2  0.316305            motto  1.047067  -0.3939  -3.4834\n",
       "10     Topic2  0.312852             free  1.047103  -0.4049  -3.4944\n",
       "12     Topic2  0.311577  new-hampshire’s  1.047117  -0.4090  -3.4985\n",
       "8      Topic2  0.310023            'live  1.047133  -0.4140  -3.5035\n",
       "13     Topic2  0.306299               or  1.047173  -0.4262  -3.5156\n",
       "14     Topic2  0.302916           that’s  1.047209  -0.4373  -3.5267\n",
       "1      Topic2  0.374145            romeo  1.571662  -0.6321  -3.3155\n",
       "\n",
       "[63 rows x 6 columns], token_table=      Topic      Freq             Term\n",
       "term                                  \n",
       "8         1  0.954988            'live\n",
       "6         1  0.955082               by\n",
       "2         1  0.637337           dagger\n",
       "2         2  0.637337           dagger\n",
       "15        2  0.958832              did\n",
       "9         1  0.955058            die',\n",
       "7         1  0.955193             died\n",
       "10        1  0.955016             free\n",
       "3         2  0.958339            happy\n",
       "16        2  0.958675               is\n",
       "0         1  0.955208           juliet\n",
       "4         2  0.958335          juliet:\n",
       "17        2  0.958754            know,\n",
       "11        1  0.955049            motto\n",
       "18        2  0.958869      new-england\n",
       "19        2  0.958877    new-hampshire\n",
       "12        1  0.955003  new-hampshire’s\n",
       "5         2  0.957764                o\n",
       "13        1  0.954952               or\n",
       "1         1  0.636269            romeo\n",
       "14        1  0.954919           that’s\n",
       "20        2  0.959024              you, R=21, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.gensim.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
